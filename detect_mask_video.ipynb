{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector model...\n",
      "[INFO] loading face mask detector model...\n",
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python detect_mask_video.py\n",
    "# importer les bibliothèques nécessaires\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from time import strftime \n",
    "import imageio\n",
    "def detect_and_predict_mask(frame, faceNet, maskNet):#construction de notre détecteurs de visages et de masques\n",
    "\t# grab the dimensions of the frame and then construct a blob\n",
    "\t# from it\n",
    "\t(h, w) = frame.shape[:2]##hauteur et largeur de l'image\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0))\n",
    "#frame: Il s'agit de l'image d'entrée que nous voulons prétraiter avant de la transmettre à notre réseau neuronal profond pour la classification.\n",
    "#scalefactor: Après avoir effectué une soustraction moyenne, nous pouvons éventuellement mettre à l'échelle nos images par un certain facteur. Cette valeur par défaut est «1.0» (c'est-à-dire pas de mise à l'échelle) mais nous pouvons également fournir une autre valeur. Il est également important de noter que scalefactor doit être 1 / \\ sigma car nous multiplions en fait les canaux d'entrée (après soustraction de la moyenne) par scalefactor.\n",
    "#size: Ici, nous fournissons la taille spatiale attendue par le réseau de neurones convolutifs. Pour la plupart des réseaux de neurones de pointe actuels, il s'agit de 224 × 224, 227 × 227 ou 299 × 299.\n",
    "#signifie: ce sont nos valeurs moyennes de soustraction. Ils peuvent être un 3-tuple des moyens RVB ou ils peuvent être une valeur unique, auquel cas la valeur fournie est soustraite de chaque canal de l'image. Si vous effectuez une soustraction moyenne, assurez-vous de fournir le 3-tuple dans l'ordre `(R, V, B)`, en particulier lorsque vous utilisez le comportement par défaut de swapRB = True.\n",
    "#swapRB: OpenCV suppose que les images sont dans l'ordre des canaux BGR; cependant, la valeur «moyenne» suppose que nous utilisons l'ordre RVB. Pour résoudre cet écart, nous pouvons permuter les canaux R et B dans l'image en définissant cette valeur sur «True». Par défaut, OpenCV effectue ce changement de chaîne pour nous.\n",
    "#La fonction cv2.dnn.blobFromImage renvoie un blob qui est notre image d'entrée après la soustraction moyenne, la normalisation et le changement de canal.\n",
    "#détection de visages\n",
    "\tfaceNet.setInput(blob)\n",
    "\tdetections = faceNet.forward()\n",
    "\n",
    "\n",
    "\tfaces = []#listes des visages\n",
    "\tlocs = []#listes des localisations où se trouvent les visages \n",
    "\tpreds = []#listes des prédictions\n",
    "\t# les détections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\n",
    "\t#filtrer les détections faibles qui seront considérés comme des erreurs\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t#calculer les coordonnées des limites de la régions d'intérêt\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
    "\t\t\t# the frame\n",
    "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
    "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
    "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
    "\t\t\tface = frame[startY:endY, startX:endX]#ROI\n",
    "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)#convert to the BGR color space\n",
    "\t\t\tface = cv2.resize(face, (224, 224))#redimensionnement\n",
    "\t\t\tface = img_to_array(face)#transformation en matrice\n",
    "\t\t\tface = preprocess_input(face)#la normalisation des valeurs des pixels en [-1, 1]\n",
    "\n",
    "\t\t\t# add the face and bounding boxes to their respective\n",
    "\t\t\t# lists\n",
    "\t\t\tfaces.append(face)\n",
    "\t\t\tlocs.append((startX, startY, endX, endY))#localisation où setrouve l'image\n",
    "\n",
    "\t# only make a predictions if at least one face was detected\n",
    "\tif len(faces) > 0:\n",
    "\t\t# for faster inference we'll make batch predictions on *all*\n",
    "\t\t# faces at the same time rather than one-by-one predictions\n",
    "\t\t# in the above `for` loop\n",
    "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
    "\t\tpreds = maskNet.predict(faces, batch_size=32)#prédire avec le maskNet model\n",
    "\treturn (locs, preds)\n",
    "\n",
    "\n",
    "\n",
    "# load our serialized face detector model from disk\n",
    "print(\"[INFO] loading face detector model...\")\n",
    "prototxtPath = os.path.sep.join([\"face_detector\", \"deploy.prototxt\"])\n",
    "weightsPath = os.path.sep.join([\"face_detector\",\n",
    "\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)#construction du détecteur de visages\n",
    "\n",
    "# load the face mask detector model from disk\n",
    "print(\"[INFO] loading face mask detector model...\")\n",
    "maskNet = load_model(\"mask_detector.model\")\n",
    "\n",
    "# Activer la webcam et la considérer comme le flux d'image d'entrée\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=1).start()\n",
    "\n",
    "nbr_vis=0\n",
    "avec=0\n",
    "sans=0\n",
    "\n",
    "while True:\n",
    "\tframe = vs.read()#pour lire la vidéo\n",
    "\tframe = imutils.resize(frame, width=400)#redimensionnement avec une largeur de 400 pixels\n",
    "\n",
    "#détections de visages et prédictions avec ou sans masques\n",
    "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "\n",
    "\tfor (box, pred) in zip(locs, preds):\n",
    "\t#calculer les coordonnées des limites de la régions d'intérêt\n",
    "\t\t(startX, startY, endX, endY) = box\n",
    "\t\t(mask, withoutMask) = pred\n",
    "\n",
    "        \n",
    "     \n",
    "\t\t# déterminer la plus grande probabilité et ajouter du texte et un rectangle au visage détecté\n",
    "\t\tif mask > withoutMask:\n",
    "\t\t    now = datetime.now().time()\n",
    "\t\t    time1=now.strftime(\"%H:%M:%S\")\n",
    "\t\t    label=\"Mask\"\n",
    "\t\t    color = (0, 255, 0)\n",
    "\t\t    avec+=1\n",
    "\t\t    label2 = \"{}: {:.2f}% \".format(label,mask* 100)\n",
    "\t\t    label3= \"{}\".format(time1)\n",
    "\t\t    cv2.putText(frame, label2, (startX, startY - 10),\n",
    "\t\t    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)                        \n",
    "\t\t    cv2.putText(frame, label3, (320,30),                        \n",
    "\t\t    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "\t\t    cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\t\telse:           \n",
    "\t\t    now = datetime.now().time()\n",
    "\t\t    time1=now.strftime(\"%H:%M:%S\")\n",
    "\t\t    label=\"No mask\"\n",
    "\t\t    color = (0, 0, 255)\n",
    "\t\t    sans+=1\n",
    "\t\t    label2 = \"{}: {:.2f}% \".format(label,withoutMask* 100)\n",
    "\t\t    label3= \"{}\".format(time1)            \n",
    "\t\t    cv2.putText(frame, label2, (startX, startY - 10),\n",
    "\t\t    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "\t\t    cv2.putText(frame, label3, (320, 30),            \n",
    "\t\t    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)   \n",
    "\t\t    cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\t\t    nom= \"sans_masque{}.PNG\".format(sans)           \n",
    "\t\t    cv2.imwrite(nom,frame)\n",
    "\t\t    time.sleep(5)  #arrêter le programme pendant 5s             \n",
    "\t\tnbr_vis=avec+sans            \n",
    "         \n",
    "\n",
    "\t\t# Mettre un texte et un contour sur les visages détectés qui diffère selon s'il est avec ou sans masque\n",
    "\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
    "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\t\tlabel1=\"Il y'a eu {} personnes sans masque \".format(sans)\n",
    "\t\tcv2.putText(frame, label1 , (50,40),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255,255), 2)\n",
    "\n",
    "\t# affichage de l'image de sortie\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(0) & 0xFF\n",
    "\n",
    "\t# pour arrêter le programme cliquer q\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n",
    "#comment régler les accents sur python?\n",
    "#quand je transforme les images en array, je le retransforme en image puis en array les 2 array n'ont pas la même valeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
